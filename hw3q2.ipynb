{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hw3, question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Updating METADATA...\n",
      "INFO: Updating Toms566...\n",
      "INFO: Computing changes...\n",
      "INFO: No packages to install, update or remove\n",
      "INFO: Nothing to be done\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "LoadError: ArgumentError: PyPlot not found in path\nwhile loading In[1], in expression starting on line 5",
     "output_type": "error",
     "traceback": [
      "LoadError: ArgumentError: PyPlot not found in path\nwhile loading In[1], in expression starting on line 5",
      "",
      " in require at ./loading.jl:233"
     ]
    }
   ],
   "source": [
    "Pkg.update()\n",
    "Pkg.add(\"ForwardDiff\")\n",
    "Pkg.add\n",
    "using ForwardDiff\n",
    "using PyPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#download(\"http://www.ats.ucla.edu/stat/data/binary.csv\", \"binary.csv\")\n",
    "A = convert(Array{Float64,2},readcsv(\"binary.csv\")[2:end,:][:,1:3]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function L(x,u)\n",
    "    a = x[1:2]; β = x[3];\n",
    "    uᵢ = u[2:3]; yᵢ = u[1]\n",
    "    return -yᵢ*(vecdot(a,uᵢ) + β) + (1-yᵢ)log(1 + exp(vecdot(a,uᵢ) + β))\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "h (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G(x,ui) = ForwardDiff.gradient(x -> L(x,ui))(x)\n",
    "H(x,ui) = ForwardDiff.hessian(x -> L(x,ui))(x)\n",
    "\n",
    "m = size(A,1)\n",
    "f(x) = sum([L(x, A[i,:]) for i in 1:m])\n",
    "g(x) = sum([G(x, A[i,:]) for i in 1:m]);\n",
    "h(x) = sum([H(x, A[i,:]) for i in 1:m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newtmin (generic function with 3 methods)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function newtmin(obj, grd, hes, x0, n,maxIts=500, optTol=1e-5)\n",
    "    # Minimize a function f using Newton’s method with modified Hessian\n",
    "    \n",
    "    # INPUT\n",
    "    # obj: a function that evaluates the objective value, gradient and Hessian at point x, i.e. (f,g,H)=obj(x)\n",
    "    # x0: starting point\n",
    "    # maxIts (optional): maximum number of iterations.\n",
    "    # optTol (optional): optimality tolerance based on: ||grad(x)|| <= optTol*||grad(x0)||\n",
    "    \n",
    "    # OUTPUT\n",
    "    # xmin: Minimum point\n",
    "    # fmin: Minimum \n",
    "    # iter: Number of iterations\n",
    "    # fgrdnorm: 2-norm of gradient of test problem at minimum point\n",
    "    \n",
    "    # Using of Newton mathod, backtracking line search with Wolfe condition\n",
    "    \n",
    "    # initialize\n",
    "    x = x0; # standard starting point\n",
    "    \n",
    "    alpha0 = 0.2; # para for backtracking line serch\n",
    "    beta0 = 0.5;\n",
    "    \n",
    "    # loop with Newton Mehtod\n",
    "    iter = 0; # number of iteration\n",
    "    gradient_L_norm2 = 1;\n",
    "    \n",
    "    Opt = Float64[]\n",
    "    \n",
    "    while gradient_L_norm2 > optTol && iter < maxIts\n",
    "        A = hes(x);\n",
    "        \n",
    "        # Non-negtive Hessian matrix check\n",
    "        min_eigval_A = minimum(eigvals(A));\n",
    "        if min_eigval_A < 0.0\n",
    "            #println(\"NEGATIVE HESSIAN MATIRX!\")\n",
    "            #break\n",
    "            beta = 1e-3;\n",
    "            if minimum(diag(hes(x))) > 0\n",
    "                tau = 0\n",
    "            else\n",
    "                tau = - minimum(diag(hes(x))) + beta;\n",
    "            end\n",
    "    \n",
    "            while min_eigval_A < 0\n",
    "                A = hes(x) + tau * eye(n)\n",
    "                min_eigval_A = minimum(eigvals(A))\n",
    "                tau = max(2 * tau, beta);\n",
    "            end   \n",
    "        end\n",
    "        \n",
    "        # compute Newton Step\n",
    "        delta_x = - inv(A) * grd(x);\n",
    "    \n",
    "        # step size by line search\n",
    "        t = 1.0;\n",
    "        f_old = obj(x); # f_old := f(x)\n",
    "\n",
    "        diff1 = 1.0; # difference of L/R Hand Side of Wolfe condition \n",
    "        diff2 = 1.0; # just initialize loop for backtracking\n",
    "        \n",
    "        c1 = 1/4; c2 = 1/2; # constent in Wolfe condition\n",
    "        \n",
    "        t = 1.0; # initial step\n",
    "        while diff1[1] > 0 && diff2[1] > 0\n",
    "            x_new = x + t * delta_x;\n",
    "            f_new = obj(x_new); # f_new := f(x + \\delta x)\n",
    "        \n",
    "            #wolfe condition\n",
    "            diff1 = f_new - f_old - c1 * alpha0 * t * (grd(x))' * delta_x;\n",
    "            diff2 = c2 * grd(x)' * delta_x - grd(x_new)' * delta_x;\n",
    "\n",
    "            t = beta0 * t;\n",
    "        end\n",
    " \n",
    "        # update\n",
    "        x = x + t * delta_x;\n",
    "        \n",
    "        #ratio = norm(grd(x)) / norm(grd(x0));\n",
    "        gradient_L_norm2 = norm(grd(x));\n",
    "        iter = iter + 1;\n",
    "        \n",
    "        # number of iteration check\n",
    "        if iter >= maxIts\n",
    "            println(\"REACH MAXIMUM ITERATION NUMBER!\")\n",
    "            break\n",
    "        end\n",
    "        \n",
    "        push!(Opt, gradient_L_norm2)\n",
    "        #println(gradient_L_norm2)\n",
    "        \n",
    "    end\n",
    "    \n",
    "    xmin = x;\n",
    "    #fmin = obj(x);\n",
    "    #fgrdnorm = norm(grd(x));\n",
    "    return (xmin,Opt,iter);\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(xmin,Opt,iter) = newtmin( x -> f(x), x -> g(x), x -> h(x), zeros(3) ,3, 500, 1e-6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "LoadError: UndefVarError: plot not defined\nwhile loading In[7], in expression starting on line 1",
     "output_type": "error",
     "traceback": [
      "LoadError: UndefVarError: plot not defined\nwhile loading In[7], in expression starting on line 1",
      ""
     ]
    }
   ],
   "source": [
    "plot(1:iter, log(Opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.0",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
